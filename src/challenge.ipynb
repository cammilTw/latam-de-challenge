{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer challenge\n",
    "\n",
    "Hola! en el siguiente notebook presento mi resoluci√≥n y mi visi√≥n respecto al challenge propuesto, tratando de adentrarme en detalle lo m√°s posible en cada una de mis respuestas, esperando que mis ideas tengan sentido respecto al rol que est√°n buscando y que coincidan con la visi√≥n que tambi√©n ustedes puedan tener para considerarme como miembro de su equipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "En consideraci√≥n a los dos enfoques propuestos (tiempo y memoria) para cada problema y que cada uno de ellos hace uso del mismo set de datos, decid√≠ establecer ciertos patrones reutilizables para cada enfoque, ya que a pesar de que los resultados de cada problema son espec√≠ficos a cada uno de ellos, la forma de acceso a los datos podr√≠a ser la misma dependiendo del enfoque, lo que me permitir√≠a enfocarme en tener menor tiempo de desarrollo y mayor tiempo para explayar mis respuestas de forma que se logren traspasar mis ideas y mi criterio en este documento.\n",
    "\n",
    "Por otro lado a nivel general, me enfoqu√© en utilizar de mayor manera posible funciones nativas de Python, salvo una excepci√≥n que facilitaba el tiempo de desarrollo en el caso de los emojis; adem√°s, trat√© de utilizar ciertas facilidades del c√≥digo de alto nivel de Python que creo que son interesantes, reutiliz√°ndolas dependiendo del enfoque a tratar y a la vez tratando de mantener la legibilidad pero sin dejar la simplicidad. Dicho esto, comenzar√© detallando cada uno de los enfoques para luego mostrar cada problema en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "En el caso del enfoque en memoria, tuve en consideraci√≥n principalmente el volumen del conjunto de datos, creo que la diferencia clave entre los dos enfoques est√° dado por el hecho de volcar todo el set de datos a la memoria o bien ir leyendo de registro en registro desde el archivo. Con esto en consideraci√≥n, en el caso de la memoria, me inclin√© a utilizar diccionarios que permitieran almacenar ids de elementos e ir manteniendo un conteo de √©stos, de forma que una vez fuera leyendo los registros uno a uno, ir detectando estos ids y aumentar los contadores de cada uno o agreg√°ndolos a los diccionarios seg√∫n corresponda.\n",
    "\n",
    "Respecto al c√≥digo, utilic√© un for loop para ir leyendo registro a registro manteniendo el archivo abierto pero sin cargarlo en memoria mediante open(), luego de parsearlos como objetos json, pude aplicar distintas metodolog√≠as dependiendo del problema a resolver para cada registro con este mismo for loop. Para el ordenamiento requerido en cada uno de los problemas, utilic√© la funci√≥n sorted, la cual permite ordenar los diccionarios anteriormente mencionados en base a los contadores almacenados para cada id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time\n",
    "\n",
    "Para este enfoque, me inclin√© en utilizar funciones de Python nativas que est√°n dise√±adas para trabajar con vol√∫menes de datos en memoria, como mencion√© en el punto anterior considero que la diferencia clave de los enfoques est√° en el volcado de datos en memoria, por lo que en este caso le√≠ el archivo completo para cargarlo en una variable str, luego en consideraci√≥n a que el archivo es un newline splitted json, utilic√© la funci√≥n splitlines para separarlo en registros y aplicar la funci√≥n de Python map para parsear cada registro como objeto json y as√≠ poder aplicar la metodolog√≠a correspondiente al problema a resolver, finalmente utilic√© el tipo de variable Counter de la librer√≠a collections de Python para retornar lo requerido por cada problema utilizando la funci√≥n most_common, la cual autom√°ticamente genera el ranking dado un conjunto de datos en memoria.\n",
    "\n",
    "Respecto a los resultados, me parece importante ser transparente respecto a ellos ya que me parecieron interesantes, al momento de hacer el benchmarking respecto a la versi√≥n enfocada en memoria, la metodolog√≠a del enfoque en memoria es levemente m√°s r√°pida que la enfocada en tiempo por lo que ciertamente hay posibles mejoras para esta versi√≥n, si tuviera que mencionar alguna, creo que se puede trabajar de manera m√°s √≥ptima la data en memoria utilizando alguna librer√≠a de procesamiento de datos que pueda manejar mejor un volumen como el utilizado para el challenge, la primera librer√≠a que pienso que podr√≠a hacer esta mejora es pandas, sin embargo como parte positiva de este resultado, se puede observar que la cantidad de llamadas a funciones en efecto es m√°s √≥ptima en la versi√≥n enfocada en tiempo comparada con la versi√≥n enfocada en memoria, lo que se puede apreciar en el detalle de cada uno de los problemas a continuaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resoluci√≥n\n",
    "\n",
    "Aplicando las metodolog√≠as descritas en los puntos anteriores, los resultados en c√≥digo para cada uno de los problemas propuestos con su respectivo detalle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code setup\n",
    "\n",
    "## Imports\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime, date  # date requerido en el primer problema\n",
    "from collections import Counter  # Requerido en la metodolog√≠a del enfoque en tiempo\n",
    "from emoji import EMOJI_DATA  # Requerido en el segundo problema\n",
    "from re import compile  # Requerido en el tercer problema\n",
    "\n",
    "\n",
    "## File path\n",
    "\n",
    "file_path = \"/Users/cammil.guzman/Downloads/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "#### Memory\n",
    "\n",
    "En la resoluci√≥n de este problema, se requer√≠a poder obtener la fecha del tweet en base al timestamp de fecha de cada registro adem√°s del usuario creador del tweet, luego en l√≠nea con la metodolog√≠a del enfoque de memoria, almacenar la fecha y la combinaci√≥n usuario/fecha en dos respectivos diccionarios para as√≠ llevar la cuenta de las cantidades de ambos, finalmente el output se reduce a un ordenamiento de estos contadores.\n",
    "\n",
    "Comenzando, se inicializan los diccionarios: uno para llevar registro de la cantidad de tweets por cada fecha y otro para registrar la cantidad de tweets por cada fecha y usuario. Luego, se puede proceder a leer el archivo mediante open() y procesar cada uno de los registros mediante un for loop, convirtiendo a un objeto json con json.loads() cada uno de ellos permitiendo obtener espec√≠ficamente la informaci√≥n necesaria minimizando el uso de memoria, obteniendo la fecha desde el valor identificado por la llave 'date' de cada tweet mediante la funci√≥n split() y la funci√≥n date.fromisoformat(), la primera de ellas permite separar un string en base a uno o m√°s caracteres, buscando en este caso el caracter \"T\" el cual denota la separaci√≥n entre la fecha y la hora, obteniendo de esta manera un string con la fecha el cual puede ser transformado en un objeto date de Python mediante la segunda funci√≥n. A continuaci√≥n, se obtiene el usuario desde el valor identificado por la llave 'username' el cual es parte del valor identificado por la llave 'user', no hay tratamiento que hacer sobre el username m√°s que almacenarlo en el contador respectivo, en consideraci√≥n a la fecha obtenida en el paso anterior. Finalmente, se ordenan estos diccionarios usando sorted, permitiendo obtener las 10 fechas donde hay mas tweets y el usuario que m√°s publicaciones tiene por cada uno de esos d√≠as."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    #Keep track of tweet count by date\n",
    "    tweetCountByDate = {}\n",
    "\n",
    "    #Keep track of user tweet count by date\n",
    "    tweetUsersByDate = {}\n",
    "\n",
    "    #Read file, by line\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            #Parse line, get username and date\n",
    "            parsedLine = json.loads(line)\n",
    "            tweetDate = date.fromisoformat(parsedLine['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
    "            tweetUsername = parsedLine['user']['username']\n",
    "\n",
    "            #Add values to trackers\n",
    "            tweetCountByDate[tweetDate] = tweetCountByDate.get(tweetDate, 0) + 1\n",
    "            tweetUsersByDate[tweetDate] = tweetUsersByDate.get(tweetDate, {})\n",
    "            tweetUsersByDate[tweetDate][tweetUsername] = tweetUsersByDate[tweetDate].get(tweetUsername, 0) + 1\n",
    "\n",
    "        #Sort dates by tweet count then for each date sort their respective users to get the top one\n",
    "        output = []\n",
    "        sortedDates = sorted(tweetCountByDate, key=lambda d: tweetCountByDate[d], reverse=True)[0:10]\n",
    "        for dateId in sortedDates:\n",
    "            mostTweetsUser = sorted(tweetUsersByDate[dateId], key=lambda username: tweetUsersByDate[dateId][username], reverse=True)[0]\n",
    "            output.append((dateId, mostTweetsUser))\n",
    "\n",
    "        return output\n",
    "    \n",
    "print(\n",
    "    q1_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "En consideraci√≥n a la metodolog√≠a del enfoque en tiempo, se busca volcar toda la informaci√≥n a la memoria para procesarla despu√©s en lotes, por lo que se requieren estructuras de datos para almacenarla y procesarla en l√≠nea con las funciones a utilizar, espec√≠ficamente la funci√≥n most_common() del tipo de datos Counter la cual requiere ejecutarse sobre una lista, en consecuencia se utilizan una lista para almacenar las fechas y un diccionario cuyas llaves son cada fecha y cuyos valores son una lista de usuarios que hayan hecho publicaciones en esa fecha, resultando en un diccionario de listas de usuarios ordenados por fecha. Finalmente, se pueden volcar los datos hacia estas variables las cuales pueden ser transformadas en variables tipo Counter sobre las cuales se puede ejecutar la funci√≥n most_common(), obteniendo los resultados esperados con c√≥digo de m√°s alto nivel sin dejar de ser nativo.\n",
    "\n",
    "En primer lugar se lee el archivo utilizando open() y read(), volcando todo el contenido del archivo en una variable tipo str, luego, se inicializan las variables mencionadas, una lista de fechas junto a un diccionario de llave fechas y valor una lista de usuarios. A continuaci√≥n, se utiliza la funci√≥n splitlines() para separar este string en l√≠neas y mediante la funci√≥n map() se aplica la funci√≥n json.loads() sobre cada una de estas lineas, convirti√©ndolas en objetos json, resultando en un iterable que puede ser procesado mediante un for loop y que por cada elemento de este se puede obtener la informaci√≥n almacenada en las llaves indicadas en la metodolog√≠a anterior de esta pregunta (['date'] y ['user']['username']), para as√≠ almacenar la fecha en la lista de fechas y el usuario en la lista respectiva del diccionario, seg√∫n la fecha obtenida. Finalmente, se puede crear una variable Counter en base a la lista de fechas y aplicando most_common(10) sobre ella se obtienen las top 10 fechas donde hay mas tweets, de la misma forma, por cada lista del diccionario se puede crear una variable tipo Counter y sobre cada una de ellas aplicar most_common(1) obteniendo el usuario que m√°s publicaciones tiene para cada fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    #Open the whole file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        rawContent = f.read()\n",
    "\n",
    "    #Keep track of dates\n",
    "    dates = []\n",
    "\n",
    "    #Keep a list for each date, holding the username for each tweet on that date\n",
    "    usersByDate = {}\n",
    "\n",
    "    #Split the file by newlines\n",
    "    for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
    "        #Parse tweet, get username and date, add them to the trackers\n",
    "        tweetDate = date.fromisoformat(tweet['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
    "        dates.append(tweetDate)\n",
    "        tweetUsername = tweet['user']['username']\n",
    "        if tweetDate in usersByDate:\n",
    "            usersByDate[tweetDate].append(tweetUsername)\n",
    "        else:\n",
    "            usersByDate[tweetDate] = [tweetUsername]\n",
    "\n",
    "    #Get the dates with most tweets\n",
    "    mostTweetedDates = Counter(dates).most_common(10)\n",
    "\n",
    "    #For the top 10 dates with most tweet counts get the most active user\n",
    "    output = []\n",
    "    for tweetDate in mostTweetedDates:\n",
    "        dateId = tweetDate[0]\n",
    "        output.append((dateId, Counter(usersByDate[dateId]).most_common(1)[0][0]))\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\n",
    "    q1_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "#### Memory\n",
    "\n",
    "Para este caso, se requer√≠a detectar y contabilizar los emojis presentes en cada una de las publicaciones lo cual se realiz√≥ mediante un diccionario cuyas llaves son los emojis y cuyos valores son el contador de apariciones de cada emoji representado con variables int, finalmente, de la misma forma que la resoluci√≥n de q1_memory, leyendo los datos registro a registro para preservar el uso de memoria y ordenando este diccionario para obtener el output.\n",
    "\n",
    "Se comienza inicializando el diccionario que almacenara los conteos de cada emoji, para luego abrir el archivo usando open() y procesar cada linea de este mediante un for loop, generando una lista de emojis presentes mediante una comprensi√≥n de lista que en primer lugar transforma cada linea en un objeto json mediante json.loads() y luego revisa cada caracter del contenido del tweet almacenado en el valor correspondiente a la llave 'content' detectando y agregando emojis a ella solamente si estos est√°n presentes en el diccionario EMOJI_DATA de la librer√≠a emoji la cual contiene una lista extensiva de emojis, luego, se puede utilizar otro for loop para iterar sobre los emojis obtenidos y ajustar los contadores respectivos seg√∫n corresponda. Finalmente, la lista de emojis es ordenada mediante la funci√≥n sorted obteniendo de esta forma los top 10 emojis m√°s usados con su respectivo conteo, output que es generado tambien por una comprensi√≥n de lista que agrega tuplas (emoji, cantidad) por cada elemento en el top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üèª', 2080), ('‚ù§', 1779), ('ü§£', 1668), ('üèΩ', 1218), ('üëá', 1108)]\n"
     ]
    }
   ],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of emojis\n",
    "    emojiCount = {}\n",
    "\n",
    "    #Read file, by line\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            #Parse line, find emojis and add them to the tracker\n",
    "            emojis = [c for c in json.loads(line)['content'] if c in EMOJI_DATA]\n",
    "            for emoji in emojis:\n",
    "                emojiCount[emoji] = emojiCount.get(emoji, 0) + 1\n",
    "\n",
    "    #Sort emojis by count\n",
    "    sortedEmojis = sorted(emojiCount, key=lambda e: emojiCount[e], reverse=True)[0:10]\n",
    "    return [(emoji, emojiCount[emoji]) for emoji in sortedEmojis]\n",
    "\n",
    "print(\n",
    "    q2_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "Al igual que en el problema anterior, la diferencia reside en procesar la informaci√≥n en lotes, por lo que en vez de utilizar contadores, se vuelcan los emojis mismos hacia una lista permitiendo utilizar una variable tipo Counter y su funci√≥n most_common() para obtener el output deseado.\n",
    "\n",
    "Luego de inicializar la lista que almacenar√° los emojis, se lee el archivo completo almacen√°ndolo en una variable de tipo str. A continuaci√≥n, de la misma manera que en el problema q1_time, se aplica json.loads() a cada elemento del iterable generado al aplicar la funci√≥n splitlines() sobre el string resultante de la lectura del archivo utilizando la funci√≥n map(). Luego, al iterar con un for loop sobre el resultado de la funci√≥n map(), se utiliza el m√©todo provisto por la variable EMOJI_DATA de la librer√≠a emoji que se indica en el enfoque de memoria del problema Q2 para agregar los emojis encontrados en el valor correspondiente a la llave 'content' de cada registro a la lista de emojis. Finalmente, se genera una variable tipo Counter en base a la lista de emojis sobre la cual se puede utilizar la funci√≥n most_common(10) obteniendo los top 10 emojis m√°s usados con su respectivo conteo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('üôè', 7286), ('üòÇ', 3072), ('üöú', 2972), ('‚úä', 2411), ('üåæ', 2363), ('üèª', 2080), ('‚ù§', 1779), ('ü§£', 1668), ('üèΩ', 1218), ('üëá', 1108)]\n"
     ]
    }
   ],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of emojis\n",
    "    tweetedEmojis = []\n",
    "\n",
    "    #Read whole file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        rawContent = f.read()\n",
    "\n",
    "    #Split the file by newlines\n",
    "    for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
    "        #Parse tweet, find emojis and add them to the tracker\n",
    "        tweetedEmojis += [c for c in tweet['content'] if c in EMOJI_DATA]\n",
    "\n",
    "    return Counter(tweetedEmojis).most_common(10)\n",
    "\n",
    "print(\n",
    "    q2_time(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "#### Memory\n",
    "\n",
    "En este √∫ltimo caso, se requiere encontrar menciones a otros usuarios dentro de cada publicaci√≥n, lo cual se puede realizar buscando el caracter '@' dentro del contenido de cada una de ellas en consideraci√≥n que son publicaciones de twitter. Al igual que en los problemas de memoria anteriores se resolvi√≥ utilizando contadores, lo cual permite utilizar la funci√≥n sorted() para ordenarlos y obtener el top 10 requerido. Para detectar el caracter '@' se utiliz√≥ la librer√≠a re de Python que permite compilar expresiones regulares y as√≠ buscar patrones de menciones de usuario dentro del contenido de la publicaci√≥n.\n",
    "\n",
    "Comenzando por inicializar el diccionario que almacenar√° los contadores de cada usuario mencionado, se prosigue con la compilaci√≥n de la expresi√≥n regular '(@\\w+)' la cual capturar√° cualquier combinaci√≥n alfanum√©rica incluyendo tambi√©n el caracter underscore, los cuales justamente comprenden los caracteres permitidos en los handlers de twitter, esta expresi√≥n regular permitir√° encontrar menciones dentro del contenido de la publicaci√≥n luego de procesar el archivo como se ha venido haciendo en la resoluci√≥n de los problemas con enfoque de memoria, es decir registro a registro con un for loop luego de abrirlos con la funci√≥n open(), luego por cada linea se utiliza la funci√≥n json.loads() para convertirla en un objeto json y aplicar la funci√≥n findall() de la expresi√≥n regular compilada en el inicio para encontrar todas las menciones dentro del contenido del tweet, espec√≠ficamente encontrado en el valor de la llave 'content' dentro del objeto json. A continuaci√≥n, por cada usuario encontrado se aumenta su contador respectivo dentro del diccionario de usuarios mencionados seg√∫n corresponda. Finalmente, utilizando la funci√≥n sorted() sobre el diccionario de usuarios mencionados se obtiene el top 10 hist√≥rico de usuarios m√°s influyentes, utilizando una comprensi√≥n de lista para generar el output requerido, correspondiendo a una lista de tuplas cuyos valores son cada usuario con su cantidad de menciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of mentioned users\n",
    "    mentionedUsersCount = {}\n",
    "\n",
    "    #Compile a regex to find '@' mentions\n",
    "    regex = compile(r'(@\\w+)')\n",
    "\n",
    "    #Read file, by line\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            #Find mentions on tweet\n",
    "            mentionedUsers = regex.findall(json.loads(line)['content'])\n",
    "\n",
    "            #Add users to the tracker\n",
    "            for mentionedUser in mentionedUsers:\n",
    "                mentionedUsersCount[mentionedUser] = mentionedUsersCount.get(mentionedUser, 0) + 1\n",
    "\n",
    "    #Sort mentioned users by count then return\n",
    "    sortedMentionedUsers = sorted(mentionedUsersCount, key=lambda e: mentionedUsersCount[e], reverse=True)[0:10]\n",
    "    return [(mentionedUser, mentionedUsersCount[mentionedUser]) for mentionedUser in sortedMentionedUsers]\n",
    "\n",
    "print(\n",
    "    q3_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "Finalmente, se utiliz√≥ tambi√©n la capacidad de las expresiones regulares para encontrar menciones dentro del contenido de cada tweet, diferenci√°ndose del enfoque en memoria respecto a la forma en la cual se lee el archivo y las funciones utilizadas para procesar los datos, los valores obtenidos y realizar el top requerido.\n",
    "\n",
    "Se inicializa una lista de usuarios mencionados que almacenar√° cada una de las menciones encontradas en los pasos siguientes, luego, se compila la expresi√≥n regular '(@\\w+)' la cual cumple la funci√≥n ya conocida. A continuaci√≥n de almacenar en una variable str el contenido del archivo, se repite el procesamiento realizado en las respuestas con enfoque de tiempo anteriores, dividiendo la variable str en una lista con la funci√≥n splitlines() cuyos elementos son procesados por la funci√≥n json.loads() a trav√©s de la funci√≥n map, resultando en un iterable que pueda ser procesado por un for loop el cual permitir√° el uso de la funci√≥n findall() de la expresi√≥n regular compilada en un inicio, con el fin de encontrar las menciones en el contenido del tweet disponible dentro del campo 'content' de cada registro, para as√≠ de esta forma agregar cada menci√≥n encontrada a la lista de usuarios mencionados. Finalmente, se transforma la lista de usuarios mencionados en una variable tipo Counter, permitiendo utilizar su funci√≥n most_common(10) para as√≠ obtener el top 10 hist√≥rico de usuarios m√°s influyentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of mentioned users\n",
    "    mentionedUsers = []\n",
    "\n",
    "    #Compile a regex to find '@' mentions\n",
    "    regex = compile(r'(@\\w+)')\n",
    "\n",
    "    #Read whole file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        rawContent = f.read()\n",
    "\n",
    "    #Split the file by newlines\n",
    "    for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
    "        #Parse tweet, find mentioned users and add them to the tracker\n",
    "        mentionedUsers += regex.findall(tweet['content'])\n",
    "\n",
    "    return Counter(mentionedUsers).most_common(10)\n",
    "\n",
    "print(\n",
    "    q3_time(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "Con el c√≥digo mencionado en consideraci√≥n, se utilizaron las librer√≠as memory-profiler y cProfile para realizar los respectivos benchmark de cada una de las respuestas.\n",
    "\n",
    "* IMPORTANTE: Setear el nombre del ejecutable de Python en el bloque de c√≥digo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark setup\n",
    "\n",
    "pythonExecutable = 'python3'\n",
    "\n",
    "## Imports\n",
    "\n",
    "import cProfile\n",
    "import subprocess\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "No hay mucho que indicar respecto al uso de memoria m√°s que destacar que efectivamente se puede apreciar un menor uso de memoria en el enfoque dirigido a este cometido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     42.8 MiB     42.8 MiB           1   @profile\n",
      "     8                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                             #Keep track of tweet count by date\n",
      "    10     42.8 MiB      0.0 MiB           1       tweetCountByDate = {}\n",
      "    11                                         \n",
      "    12                                             #Keep track of user tweet count by date\n",
      "    13     42.8 MiB      0.0 MiB           1       tweetUsersByDate = {}\n",
      "    14                                         \n",
      "    15                                             #Read file, by line\n",
      "    16     42.8 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    17     49.4 MiB      0.6 MiB      117408           for line in f:\n",
      "    18                                                     #Parse line, get username and date\n",
      "    19     49.4 MiB      3.5 MiB      117407               parsedLine = json.loads(line)\n",
      "    20     49.4 MiB      0.0 MiB      117407               tweetDate = date.fromisoformat(parsedLine['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
      "    21     49.4 MiB      0.0 MiB      117407               tweetUsername = parsedLine['user']['username']\n",
      "    22                                         \n",
      "    23                                                     #Add values to trackers\n",
      "    24     49.4 MiB      0.0 MiB      117407               tweetCountByDate[tweetDate] = tweetCountByDate.get(tweetDate, 0) + 1\n",
      "    25     49.4 MiB      0.0 MiB      117407               tweetUsersByDate[tweetDate] = tweetUsersByDate.get(tweetDate, {})\n",
      "    26     49.4 MiB      2.5 MiB      117407               tweetUsersByDate[tweetDate][tweetUsername] = tweetUsersByDate[tweetDate].get(tweetUsername, 0) + 1\n",
      "    27                                         \n",
      "    28                                                 #Sort dates by tweet count then for each date sort their respective users to get the top one\n",
      "    29     49.4 MiB      0.0 MiB           1           output = []\n",
      "    30     49.4 MiB      0.0 MiB          27           sortedDates = sorted(tweetCountByDate, key=lambda d: tweetCountByDate[d], reverse=True)[0:10]\n",
      "    31     49.5 MiB      0.0 MiB          11           for dateId in sortedDates:\n",
      "    32     49.5 MiB      0.1 MiB       88328               mostTweetsUser = sorted(tweetUsersByDate[dateId], key=lambda username: tweetUsersByDate[dateId][username], reverse=True)[0]\n",
      "    33     49.5 MiB      0.0 MiB          10               output.append((dateId, mostTweetsUser))\n",
      "    34                                         \n",
      "    35     49.5 MiB      0.0 MiB           1           return output\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_memory\n",
    "\n",
    "q1MemoryResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q1_memory.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q1MemoryResult.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     42.3 MiB     42.3 MiB           1   @profile\n",
      "     9                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             #Open the whole file\n",
      "    11     42.3 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    12    431.1 MiB    388.8 MiB           1           rawContent = f.read()\n",
      "    13                                         \n",
      "    14                                             #Keep track of dates\n",
      "    15    431.1 MiB      0.0 MiB           1       dates = []\n",
      "    16                                         \n",
      "    17                                             #Keep a list for each date, holding the username for each tweet on that date\n",
      "    18    431.1 MiB      0.0 MiB           1       usersByDate = {}\n",
      "    19                                         \n",
      "    20                                             #Split the file by newlines\n",
      "    21    870.3 MiB     29.7 MiB      117408       for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
      "    22                                                 #Parse tweet, get username and date, add them to the trackers\n",
      "    23    870.3 MiB      0.0 MiB      117407           tweetDate = date.fromisoformat(tweet['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
      "    24    870.3 MiB      0.9 MiB      117407           dates.append(tweetDate)\n",
      "    25    870.3 MiB      0.0 MiB      117407           tweetUsername = tweet['user']['username']\n",
      "    26    870.3 MiB      0.0 MiB      117407           if tweetDate in usersByDate:\n",
      "    27    870.3 MiB      0.5 MiB      117394               usersByDate[tweetDate].append(tweetUsername)\n",
      "    28                                                 else:\n",
      "    29    868.9 MiB      0.0 MiB          13               usersByDate[tweetDate] = [tweetUsername]\n",
      "    30                                         \n",
      "    31                                             #Get the dates with most tweets\n",
      "    32    462.3 MiB   -408.0 MiB           1       mostTweetedDates = Counter(dates).most_common(10)\n",
      "    33                                         \n",
      "    34                                             #For the top 10 dates with most tweet counts get the most active user\n",
      "    35    462.3 MiB      0.0 MiB           1       output = []\n",
      "    36    462.8 MiB      0.0 MiB          11       for tweetDate in mostTweetedDates:\n",
      "    37    462.8 MiB      0.0 MiB          10           dateId = tweetDate[0]\n",
      "    38    462.8 MiB      0.6 MiB          10           output.append((dateId, Counter(usersByDate[dateId]).most_common(1)[0][0]))\n",
      "    39                                         \n",
      "    40    462.8 MiB      0.0 MiB           1       return output\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_time\n",
    "\n",
    "q1TimeResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q1_time.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q1TimeResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     52.9 MiB     52.9 MiB           1   @profile\n",
      "     8                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     9                                             #Keep track of emojis\n",
      "    10     52.9 MiB      0.0 MiB           1       emojiCount = {}\n",
      "    11                                         \n",
      "    12                                             #Read file, by line\n",
      "    13     52.9 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    14     54.3 MiB      0.9 MiB      117408           for line in f:\n",
      "    15                                                     #Parse line, find emojis and add them to the tracker\n",
      "    16     54.3 MiB      0.4 MiB    17386304               emojis = [c for c in json.loads(line)['content'] if c in EMOJI_DATA]\n",
      "    17     54.3 MiB      0.0 MiB      163043               for emoji in emojis:\n",
      "    18     54.3 MiB      0.0 MiB       45636                   emojiCount[emoji] = emojiCount.get(emoji, 0) + 1\n",
      "    19                                         \n",
      "    20                                             #Sort emojis by count\n",
      "    21     54.3 MiB      0.0 MiB        1283       sortedEmojis = sorted(emojiCount, key=lambda e: emojiCount[e], reverse=True)[0:10]\n",
      "    22     54.3 MiB      0.0 MiB          13       return [(emoji, emojiCount[emoji]) for emoji in sortedEmojis]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_memory\n",
    "\n",
    "q2MemoryResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q2_memory.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q2MemoryResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     53.2 MiB     53.2 MiB           1   @profile\n",
      "     9                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    10                                             #Keep track of emojis\n",
      "    11     53.2 MiB      0.0 MiB           1       tweetedEmojis = []\n",
      "    12                                         \n",
      "    13                                             #Read whole file\n",
      "    14     53.2 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    15    435.6 MiB    382.5 MiB           1           rawContent = f.read()\n",
      "    16                                         \n",
      "    17                                             #Split the file by newlines\n",
      "    18    863.5 MiB -2858109.1 MiB      117408       for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
      "    19                                                 #Parse tweet, find emojis and add them to the tracker\n",
      "    20    863.5 MiB -408950014.3 MiB    17386304           tweetedEmojis += [c for c in tweet['content'] if c in EMOJI_DATA]\n",
      "    21                                         \n",
      "    22    396.6 MiB   -467.0 MiB           1       return Counter(tweetedEmojis).most_common(10)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_time\n",
    "\n",
    "q2TimeResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q2_time.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q2TimeResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     42.3 MiB     42.3 MiB           1   @profile\n",
      "     8                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     9                                             #Keep track of mentioned users\n",
      "    10     42.3 MiB      0.0 MiB           1       mentionedUsersCount = {}\n",
      "    11                                         \n",
      "    12                                             #Compile a regex to find '@' mentions\n",
      "    13     42.3 MiB      0.0 MiB           1       regex = compile(r'(@\\w+)')\n",
      "    14                                         \n",
      "    15                                             #Read file, by line\n",
      "    16     42.3 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    17     45.7 MiB      1.0 MiB      117408           for line in f:\n",
      "    18                                                     #Find mentions on tweet\n",
      "    19     45.7 MiB      1.3 MiB      117407               mentionedUsers = regex.findall(json.loads(line)['content'])\n",
      "    20                                         \n",
      "    21                                                     #Add users to the tracker\n",
      "    22     45.7 MiB      0.0 MiB      221473               for mentionedUser in mentionedUsers:\n",
      "    23     45.7 MiB      1.1 MiB      104066                   mentionedUsersCount[mentionedUser] = mentionedUsersCount.get(mentionedUser, 0) + 1\n",
      "    24                                         \n",
      "    25                                             #Sort mentioned users by count then return\n",
      "    26     46.0 MiB      0.2 MiB       31351       sortedMentionedUsers = sorted(mentionedUsersCount, key=lambda e: mentionedUsersCount[e], reverse=True)[0:10]\n",
      "    27     46.0 MiB      0.0 MiB          13       return [(mentionedUser, mentionedUsersCount[mentionedUser]) for mentionedUser in sortedMentionedUsers]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_memory\n",
    "\n",
    "q3MemoryResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q3_memory.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q3MemoryResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     42.7 MiB     42.7 MiB           1   @profile\n",
      "     9                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    10                                             #Keep track of mentioned users\n",
      "    11     42.7 MiB      0.0 MiB           1       mentionedUsers = []\n",
      "    12                                         \n",
      "    13                                             #Compile a regex to find '@' mentions\n",
      "    14     42.7 MiB      0.0 MiB           1       regex = compile(r'(@\\w+)')\n",
      "    15                                         \n",
      "    16                                             #Read whole file\n",
      "    17     42.7 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    18    431.4 MiB    388.8 MiB           1           rawContent = f.read()\n",
      "    19                                         \n",
      "    20                                             #Split the file by newlines\n",
      "    21    865.4 MiB     25.0 MiB      117408       for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
      "    22                                                 #Parse tweet, find mentioned users and add them to the tracker\n",
      "    23    865.4 MiB      1.0 MiB      117407           mentionedUsers += regex.findall(tweet['content'])\n",
      "    24                                         \n",
      "    25    458.6 MiB   -406.8 MiB           1       return Counter(mentionedUsers).most_common(10)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_time\n",
    "\n",
    "q3TimeResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q3_time.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q3TimeResult.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time\n",
    "\n",
    "Si bien los resultados relativos al tiempo dependen del procesamiento realizado de cara a resolver cada uno de los problemas propuestos, cabe destacar que se puede verificar lo indicado en el apartado Time del Overview de este documento, donde se puede apreciar que la metodolog√≠a planteada para resolver los problemas con enfoque de tiempo se demora levemente m√°s que la metodolog√≠a propuesta para resolver los problemas con enfoque en uso de memoria, lo cual podr√≠a ser un indicio que un mejor criterio para crear c√≥digo de mayor calidad ser√≠a minimizar el uso de funciones de alto nivel como se realiz√≥ en este caso, espec√≠ficamente map(), splitlines() y most_common(), ya que como se puede apreciar en la metodolog√≠a del enfoque en memoria, se pueden tener mejores tiempos con un mucho menor uso de recursos. Por otro lado, es importante destacar la disminuci√≥n de llamadas a funciones que se obtiene con la metodolog√≠a con enfoque de tiempo que se puede verificar en los benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1904852 function calls in 2.284 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.509    0.509    2.284    2.284 618638651.py:1(q1_memory)\n",
      "       13    0.000    0.000    0.000    0.000 618638651.py:23(<lambda>)\n",
      "    44159    0.002    0.000    0.002    0.000 618638651.py:25(<lambda>)\n",
      "        1    0.000    0.000    2.284    2.284 <string>:1(<module>)\n",
      "   117407    0.045    0.000    1.680    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.014    0.000    0.039    0.000 codecs.py:319(decode)\n",
      "   117407    0.069    0.000    1.619    0.000 decoder.py:332(decode)\n",
      "   117407    1.497    0.000    1.497    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "    49772    0.025    0.000    0.025    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.000    0.000    2.284    2.284 {built-in method builtins.exec}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "       11    0.004    0.000    0.006    0.001 {built-in method builtins.sorted}\n",
      "   117407    0.007    0.000    0.007    0.000 {built-in method fromisoformat}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.010    0.000    0.010    0.000 {method 'end' of 're.Match' objects}\n",
      "   352221    0.030    0.000    0.030    0.000 {method 'get' of 'dict' objects}\n",
      "   234814    0.038    0.000    0.038    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'split' of 'str' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_memory\n",
    "cProfile.run('q1_memory(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1643837 function calls in 2.683 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.199    0.199    2.677    2.677 3097206275.py:1(q1_time)\n",
      "        1    0.007    0.007    2.683    2.683 <string>:1(<module>)\n",
      "   117407    0.041    0.000    1.666    0.000 __init__.py:299(loads)\n",
      "       11    0.000    0.000    0.010    0.001 __init__.py:581(__init__)\n",
      "       11    0.000    0.000    0.001    0.000 __init__.py:600(most_common)\n",
      "       11    0.000    0.000    0.010    0.001 __init__.py:649(update)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "       11    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "        1    0.000    0.000    0.068    0.068 codecs.py:319(decode)\n",
      "   117407    0.065    0.000    1.611    0.000 decoder.py:332(decode)\n",
      "   117407    1.498    0.000    1.498    0.000 decoder.py:343(raw_decode)\n",
      "       11    0.000    0.000    0.001    0.000 heapq.py:521(nlargest)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:563(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:577(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 interactiveshell.py:301(_modified_open)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.068    0.068    0.068    0.068 {built-in method _codecs.utf_8_decode}\n",
      "       11    0.010    0.001    0.010    0.001 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _heapq.heapreplace}\n",
      "        1    0.000    0.000    2.683    2.683 {built-in method builtins.exec}\n",
      "   117418    0.005    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "       10    0.001    0.000    0.001    0.000 {built-in method builtins.max}\n",
      "   117407    0.007    0.000    0.007    0.000 {built-in method fromisoformat}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "   234811    0.009    0.000    0.009    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.010    0.000    0.010    0.000 {method 'end' of 're.Match' objects}\n",
      "       11    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "   234814    0.032    0.000    0.032    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        1    0.466    0.466    0.534    0.534 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'split' of 'str' objects}\n",
      "        1    0.238    0.238    0.238    0.238 {method 'splitlines' of 'str' objects}\n",
      "   117407    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_time\n",
    "cProfile.run('q1_time(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1437310 function calls in 2.665 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.437    0.437    2.665    2.665 2931744933.py:1(q2_memory)\n",
      "      641    0.000    0.000    0.000    0.000 2931744933.py:14(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 2931744933.py:15(<listcomp>)\n",
      "   117407    0.515    0.000    0.515    0.000 2931744933.py:9(<listcomp>)\n",
      "        1    0.000    0.000    2.665    2.665 <string>:1(<module>)\n",
      "   117407    0.048    0.000    1.670    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.014    0.000    0.039    0.000 codecs.py:319(decode)\n",
      "   117407    0.067    0.000    1.607    0.000 decoder.py:332(decode)\n",
      "   117407    1.490    0.000    1.490    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "    49772    0.025    0.000    0.025    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.000    0.000    2.665    2.665 {built-in method builtins.exec}\n",
      "   117407    0.004    0.000    0.004    0.000 {built-in method builtins.isinstance}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "    45636    0.003    0.000    0.003    0.000 {method 'get' of 'dict' objects}\n",
      "   234814    0.037    0.000    0.037    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_memory\n",
    "cProfile.run('q2_memory(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1291513 function calls in 2.971 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.156    0.156    2.966    2.966 2183131300.py:1(q2_time)\n",
      "   117407    0.499    0.000    0.499    0.000 2183131300.py:12(<listcomp>)\n",
      "        1    0.005    0.005    2.971    2.971 <string>:1(<module>)\n",
      "   117407    0.045    0.000    1.624    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:581(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:600(most_common)\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:649(update)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "        1    0.000    0.000    0.054    0.054 codecs.py:319(decode)\n",
      "   117407    0.062    0.000    1.565    0.000 decoder.py:332(decode)\n",
      "   117407    1.459    0.000    1.459    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:521(nlargest)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:563(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:577(<listcomp>)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.054    0.054    0.054    0.054 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method _heapq.heapreplace}\n",
      "        1    0.000    0.000    2.971    2.971 {built-in method builtins.exec}\n",
      "   117408    0.004    0.000    0.004    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "   234814    0.031    0.000    0.031    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        1    0.386    0.386    0.440    0.440 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "        1    0.244    0.244    0.244    0.244 {method 'splitlines' of 'str' objects}\n",
      "   117407    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_time\n",
    "cProfile.run('q2_time(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1510777 function calls in 2.197 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.438    0.438    2.197    2.197 717031050.py:1(q3_memory)\n",
      "    15675    0.001    0.000    0.001    0.000 717031050.py:19(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 717031050.py:20(<listcomp>)\n",
      "        1    0.000    0.000    2.197    2.197 <string>:1(<module>)\n",
      "   117407    0.044    0.000    1.663    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.014    0.000    0.038    0.000 codecs.py:319(decode)\n",
      "   117407    0.066    0.000    1.604    0.000 decoder.py:332(decode)\n",
      "   117407    1.487    0.000    1.487    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:250(compile)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:289(_compile)\n",
      "    49772    0.025    0.000    0.025    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.000    0.000    2.197    2.197 {built-in method builtins.exec}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.002    0.002    0.002    0.002 {built-in method builtins.sorted}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.035    0.000    0.035    0.000 {method 'findall' of 're.Pattern' objects}\n",
      "   104066    0.020    0.000    0.020    0.000 {method 'get' of 'dict' objects}\n",
      "   234814    0.036    0.000    0.036    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_memory\n",
    "cProfile.run('q3_memory(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1291529 function calls in 2.617 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.146    0.146    2.612    2.612 2747271002.py:1(q3_time)\n",
      "        1    0.005    0.005    2.617    2.617 <string>:1(<module>)\n",
      "   117407    0.040    0.000    1.611    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.006    0.006 __init__.py:581(__init__)\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:600(most_common)\n",
      "        1    0.000    0.000    0.006    0.006 __init__.py:649(update)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "        1    0.000    0.000    0.075    0.075 codecs.py:319(decode)\n",
      "   117407    0.062    0.000    1.557    0.000 decoder.py:332(decode)\n",
      "   117407    1.452    0.000    1.452    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.001    0.001    0.001    0.001 heapq.py:521(nlargest)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:563(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:577(<listcomp>)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:250(compile)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:289(_compile)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.075    0.075    0.075    0.075 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.006    0.006    0.006    0.006 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method _heapq.heapreplace}\n",
      "        1    0.000    0.000    2.617    2.617 {built-in method builtins.exec}\n",
      "   117409    0.004    0.000    0.004    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.001    0.001    0.001    0.001 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.032    0.000    0.032    0.000 {method 'findall' of 're.Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "   234814    0.030    0.000    0.030    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        1    0.487    0.487    0.562    0.562 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "        1    0.252    0.252    0.252    0.252 {method 'splitlines' of 'str' objects}\n",
      "   117407    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_time\n",
    "cProfile.run('q3_time(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud\n",
    "\n",
    "La simplicidad y modularidad del c√≥digo provisto permite un f√°cil despliegue en la nube, al ser cada funci√≥n un script simple, se puede desplegar sin mucho esfuerzo en un servicio como GCP Cloud Functions, o bien, puede ser agregado a una imagen Docker con la finalidad de ejecutarlo en alg√∫n servicio de despliegue de plataformas Kubernetes, lo cual est√° disponible en todos los proveedores Cloud con mayor presencia en el mercado, inclusive, se puede utilizar tambi√©n una imagen Docker para desplegar el c√≥digo en servicios m√°s espec√≠ficos como lo es GCP Cloud Run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cierre\n",
    "\n",
    "Para finalizar, a nivel personal, me pareci√≥ una experiencia importante darme cuenta en el proceso de realizar el challenge que hab√≠a obtenido resultados dispares a lo esperado, y dado este caso, prefer√≠ explayarme respecto a estos hallazgos en vez de tratar de llegar a un resultado espec√≠fico mediante el c√≥digo, creo que es una experiencia que ameritaba ser escrita y que me permiti√≥ volcar mis ideas de una forma que me deja conforme, de la misma forma, espero haber logrado traspasar estas ideas de manera clara tanto en sus descripciones como en el c√≥digo mismo.\n",
    "\n",
    "Gracias!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
