{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer challenge\n",
    "\n",
    "Hola! en el siguiente notebook presento mi resolución y mi visión respecto al challenge propuesto, tratando de adentrarme en detalle lo más posible en cada una de mis respuestas, esperando que mis ideas tengan sentido respecto al rol que están buscando y que coincidan con la visión que también ustedes puedan tener para considerarme como miembro de su equipo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "En consideración a los dos enfoques propuestos (tiempo y memoria) para cada problema y que cada uno de ellos hace uso del mismo set de datos, decidí establecer ciertos patrones reutilizables para cada enfoque, ya que a pesar de que los resultados de cada problema son específicos a cada uno de ellos, la forma de acceso a los datos podría ser la misma dependiendo del enfoque, lo que me permitiría enfocarme en tener menor tiempo de desarrollo y mayor tiempo para explayar mis respuestas de forma que se logren traspasar mis ideas y mi criterio en este documento.\n",
    "\n",
    "Por otro lado a nivel general, me enfoqué en utilizar de mayor manera posible funciones nativas de Python, salvo una excepción que facilitaba el tiempo de desarrollo en el caso de los emojis; además, traté de utilizar ciertas facilidades del código de alto nivel de Python que creo que son interesantes, reutilizándolas dependiendo del enfoque a tratar y a la vez tratando de mantener la legibilidad pero sin dejar la simplicidad. Dicho esto, comenzaré detallando cada uno de los enfoques para luego mostrar cada problema en detalle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "En el caso del enfoque en memoria, tuve en consideración principalmente el volumen del conjunto de datos, creo que la diferencia clave entre los dos enfoques está dado por el hecho de volcar todo el set de datos a la memoria o bien ir leyendo de registro en registro desde el archivo. Con esto en consideración, en el caso de la memoria, me incliné a utilizar diccionarios que permitieran almacenar ids de elementos e ir manteniendo un conteo de éstos, de forma que una vez fuera leyendo los registros uno a uno, ir detectando estos ids y aumentar los contadores de cada uno o agregándolos a los diccionarios según corresponda.\n",
    "\n",
    "Respecto al código, utilicé un for loop para ir leyendo registro a registro manteniendo el archivo abierto pero sin cargarlo en memoria mediante open(), luego de parsearlos como objetos json, pude aplicar distintas metodologías dependiendo del problema a resolver para cada registro con este mismo for loop. Para el ordenamiento requerido en cada uno de los problemas, utilicé la función sorted, la cual permite ordenar los diccionarios anteriormente mencionados en base a los contadores almacenados para cada id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time\n",
    "\n",
    "Para este enfoque, me incliné en utilizar funciones de Python nativas que están diseñadas para trabajar con volúmenes de datos en memoria, como mencioné en el punto anterior considero que la diferencia clave de los enfoques está en el volcado de datos en memoria, por lo que en este caso leí el archivo completo para cargarlo en una variable str, luego en consideración a que el archivo es un newline splitted json, utilicé la función splitlines para separarlo en registros y aplicar la función de Python map para parsear cada registro como objeto json y así poder aplicar la metodología correspondiente al problema a resolver, finalmente utilicé el tipo de variable Counter de la librería collections de Python para retornar lo requerido por cada problema utilizando la función most_common, la cual automáticamente genera el ranking dado un conjunto de datos en memoria.\n",
    "\n",
    "Respecto a los resultados, me parece importante ser transparente respecto a ellos ya que me parecieron interesantes, al momento de hacer el benchmarking respecto a la versión enfocada en memoria, la metodología del enfoque en memoria es levemente más rápida que la enfocada en tiempo por lo que ciertamente hay posibles mejoras para esta versión, si tuviera que mencionar alguna, creo que se puede trabajar de manera más óptima la data en memoria utilizando alguna librería de procesamiento de datos que pueda manejar mejor un volumen como el utilizado para el challenge, la primera librería que pienso que podría hacer esta mejora es pandas, sin embargo como parte positiva de este resultado, se puede observar que la cantidad de llamadas a funciones en efecto es más óptima en la versión enfocada en tiempo comparada con la versión enfocada en memoria, lo que se puede apreciar en el detalle de cada uno de los problemas a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolución\n",
    "\n",
    "Aplicando las metodologías descritas en los puntos anteriores, los resultados en código para cada uno de los problemas propuestos con su respectivo detalle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code setup\n",
    "\n",
    "## Imports\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import List, Tuple\n",
    "from datetime import datetime, date  # date requerido en el primer problema\n",
    "from collections import Counter  # Requerido en la metodología del enfoque en tiempo\n",
    "from emoji import EMOJI_DATA  # Requerido en el segundo problema\n",
    "from re import compile  # Requerido en el tercer problema\n",
    "\n",
    "\n",
    "## File path\n",
    "\n",
    "file_path = \"/Users/cammil.guzman/Downloads/farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "#### Memory\n",
    "\n",
    "En la resolución de este problema, se requería poder obtener la fecha del tweet en base al timestamp de fecha de cada registro además del usuario creador del tweet, luego en línea con la metodología del enfoque de memoria, almacenar la fecha y la combinación usuario/fecha en dos respectivos diccionarios para así llevar la cuenta de las cantidades de ambos, finalmente el output se reduce a un ordenamiento de estos contadores.\n",
    "\n",
    "Comenzando, se inicializan los diccionarios: uno para llevar registro de la cantidad de tweets por cada fecha y otro para registrar la cantidad de tweets por cada fecha y usuario. Luego, se puede proceder a leer el archivo mediante open() y procesar cada uno de los registros mediante un for loop, convirtiendo a un objeto json con json.loads() cada uno de ellos permitiendo obtener específicamente la información necesaria minimizando el uso de memoria, obteniendo la fecha desde el valor identificado por la llave 'date' de cada tweet mediante la función split() y la función date.fromisoformat(), la primera de ellas permite separar un string en base a uno o más caracteres, buscando en este caso el caracter \"T\" el cual denota la separación entre la fecha y la hora, obteniendo de esta manera un string con la fecha el cual puede ser transformado en un objeto date de Python mediante la segunda función. A continuación, se obtiene el usuario desde el valor identificado por la llave 'username' el cual es parte del valor identificado por la llave 'user', no hay tratamiento que hacer sobre el username más que almacenarlo en el contador respectivo, en consideración a la fecha obtenida en el paso anterior. Finalmente, se ordenan estos diccionarios usando sorted, permitiendo obtener las 10 fechas donde hay mas tweets y el usuario que más publicaciones tiene por cada uno de esos días."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    #Keep track of tweet count by date\n",
    "    tweetCountByDate = {}\n",
    "\n",
    "    #Keep track of user tweet count by date\n",
    "    tweetUsersByDate = {}\n",
    "\n",
    "    #Read file, by line\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            #Parse line, get username and date\n",
    "            parsedLine = json.loads(line)\n",
    "            tweetDate = date.fromisoformat(parsedLine['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
    "            tweetUsername = parsedLine['user']['username']\n",
    "\n",
    "            #Add values to trackers\n",
    "            tweetCountByDate[tweetDate] = tweetCountByDate.get(tweetDate, 0) + 1\n",
    "            tweetUsersByDate[tweetDate] = tweetUsersByDate.get(tweetDate, {})\n",
    "            tweetUsersByDate[tweetDate][tweetUsername] = tweetUsersByDate[tweetDate].get(tweetUsername, 0) + 1\n",
    "\n",
    "        #Sort dates by tweet count then for each date sort their respective users to get the top one\n",
    "        output = []\n",
    "        sortedDates = sorted(tweetCountByDate, key=lambda d: tweetCountByDate[d], reverse=True)[0:10]\n",
    "        for dateId in sortedDates:\n",
    "            mostTweetsUser = sorted(tweetUsersByDate[dateId], key=lambda username: tweetUsersByDate[dateId][username], reverse=True)[0]\n",
    "            output.append((dateId, mostTweetsUser))\n",
    "\n",
    "        return output\n",
    "    \n",
    "print(\n",
    "    q1_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "En consideración a la metodología del enfoque en tiempo, se busca volcar toda la información a la memoria para procesarla después en lotes, por lo que se requieren estructuras de datos para almacenarla y procesarla en línea con las funciones a utilizar, específicamente la función most_common() del tipo de datos Counter la cual requiere ejecutarse sobre una lista, en consecuencia se utilizan una lista para almacenar las fechas y un diccionario cuyas llaves son cada fecha y cuyos valores son una lista de usuarios que hayan hecho publicaciones en esa fecha, resultando en un diccionario de listas de usuarios ordenados por fecha. Finalmente, se pueden volcar los datos hacia estas variables las cuales pueden ser transformadas en variables tipo Counter sobre las cuales se puede ejecutar la función most_common(), obteniendo los resultados esperados con código de más alto nivel sin dejar de ser nativo.\n",
    "\n",
    "En primer lugar se lee el archivo utilizando open() y read(), volcando todo el contenido del archivo en una variable tipo str, luego, se inicializan las variables mencionadas, una lista de fechas junto a un diccionario de llave fechas y valor una lista de usuarios. A continuación, se utiliza la función splitlines() para separar este string en líneas y mediante la función map() se aplica la función json.loads() sobre cada una de estas lineas, convirtiéndolas en objetos json, resultando en un iterable que puede ser procesado mediante un for loop y que por cada elemento de este se puede obtener la información almacenada en las llaves indicadas en la metodología anterior de esta pregunta (['date'] y ['user']['username']), para así almacenar la fecha en la lista de fechas y el usuario en la lista respectiva del diccionario, según la fecha obtenida. Finalmente, se puede crear una variable Counter en base a la lista de fechas y aplicando most_common(10) sobre ella se obtienen las top 10 fechas donde hay mas tweets, de la misma forma, por cada lista del diccionario se puede crear una variable tipo Counter y sobre cada una de ellas aplicar most_common(1) obteniendo el usuario que más publicaciones tiene para cada fecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
    "    #Open the whole file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        rawContent = f.read()\n",
    "\n",
    "    #Keep track of dates\n",
    "    dates = []\n",
    "\n",
    "    #Keep a list for each date, holding the username for each tweet on that date\n",
    "    usersByDate = {}\n",
    "\n",
    "    #Split the file by newlines\n",
    "    for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
    "        #Parse tweet, get username and date, add them to the trackers\n",
    "        tweetDate = date.fromisoformat(tweet['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
    "        dates.append(tweetDate)\n",
    "        tweetUsername = tweet['user']['username']\n",
    "        if tweetDate in usersByDate:\n",
    "            usersByDate[tweetDate].append(tweetUsername)\n",
    "        else:\n",
    "            usersByDate[tweetDate] = [tweetUsername]\n",
    "\n",
    "    #Get the dates with most tweets\n",
    "    mostTweetedDates = Counter(dates).most_common(10)\n",
    "\n",
    "    #For the top 10 dates with most tweet counts get the most active user\n",
    "    output = []\n",
    "    for tweetDate in mostTweetedDates:\n",
    "        dateId = tweetDate[0]\n",
    "        output.append((dateId, Counter(usersByDate[dateId]).most_common(1)[0][0]))\n",
    "\n",
    "    return output\n",
    "\n",
    "print(\n",
    "    q1_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "#### Memory\n",
    "\n",
    "Para este caso, se requería detectar y contabilizar los emojis presentes en cada una de las publicaciones lo cual se realizó mediante un diccionario cuyas llaves son los emojis y cuyos valores son el contador de apariciones de cada emoji representado con variables int, finalmente, de la misma forma que la resolución de q1_memory, leyendo los datos registro a registro para preservar el uso de memoria y ordenando este diccionario para obtener el output.\n",
    "\n",
    "Se comienza inicializando el diccionario que almacenara los conteos de cada emoji, para luego abrir el archivo usando open() y procesar cada linea de este mediante un for loop, generando una lista de emojis presentes mediante una comprensión de lista que en primer lugar transforma cada linea en un objeto json mediante json.loads() y luego revisa cada caracter del contenido del tweet almacenado en el valor correspondiente a la llave 'content' detectando y agregando emojis a ella solamente si estos están presentes en el diccionario EMOJI_DATA de la librería emoji la cual contiene una lista extensiva de emojis, luego, se puede utilizar otro for loop para iterar sobre los emojis obtenidos y ajustar los contadores respectivos según corresponda. Finalmente, la lista de emojis es ordenada mediante la función sorted obteniendo de esta forma los top 10 emojis más usados con su respectivo conteo, output que es generado tambien por una comprensión de lista que agrega tuplas (emoji, cantidad) por cada elemento en el top 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('✊', 2411), ('🌾', 2363), ('🏻', 2080), ('❤', 1779), ('🤣', 1668), ('🏽', 1218), ('👇', 1108)]\n"
     ]
    }
   ],
   "source": [
    "def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of emojis\n",
    "    emojiCount = {}\n",
    "\n",
    "    #Read file, by line\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            #Parse line, find emojis and add them to the tracker\n",
    "            emojis = [c for c in json.loads(line)['content'] if c in EMOJI_DATA]\n",
    "            for emoji in emojis:\n",
    "                emojiCount[emoji] = emojiCount.get(emoji, 0) + 1\n",
    "\n",
    "    #Sort emojis by count\n",
    "    sortedEmojis = sorted(emojiCount, key=lambda e: emojiCount[e], reverse=True)[0:10]\n",
    "    return [(emoji, emojiCount[emoji]) for emoji in sortedEmojis]\n",
    "\n",
    "print(\n",
    "    q2_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "Al igual que en el problema anterior, la diferencia reside en procesar la información en lotes, por lo que en vez de utilizar contadores, se vuelcan los emojis mismos hacia una lista permitiendo utilizar una variable tipo Counter y su función most_common() para obtener el output deseado.\n",
    "\n",
    "Luego de inicializar la lista que almacenará los emojis, se lee el archivo completo almacenándolo en una variable de tipo str. A continuación, de la misma manera que en el problema q1_time, se aplica json.loads() a cada elemento del iterable generado al aplicar la función splitlines() sobre el string resultante de la lectura del archivo utilizando la función map(). Luego, al iterar con un for loop sobre el resultado de la función map(), se utiliza el método provisto por la variable EMOJI_DATA de la librería emoji que se indica en el enfoque de memoria del problema Q2 para agregar los emojis encontrados en el valor correspondiente a la llave 'content' de cada registro a la lista de emojis. Finalmente, se genera una variable tipo Counter en base a la lista de emojis sobre la cual se puede utilizar la función most_common(10) obteniendo los top 10 emojis más usados con su respectivo conteo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('🙏', 7286), ('😂', 3072), ('🚜', 2972), ('✊', 2411), ('🌾', 2363), ('🏻', 2080), ('❤', 1779), ('🤣', 1668), ('🏽', 1218), ('👇', 1108)]\n"
     ]
    }
   ],
   "source": [
    "def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of emojis\n",
    "    tweetedEmojis = []\n",
    "\n",
    "    #Read whole file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        rawContent = f.read()\n",
    "\n",
    "    #Split the file by newlines\n",
    "    for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
    "        #Parse tweet, find emojis and add them to the tracker\n",
    "        tweetedEmojis += [c for c in tweet['content'] if c in EMOJI_DATA]\n",
    "\n",
    "    return Counter(tweetedEmojis).most_common(10)\n",
    "\n",
    "print(\n",
    "    q2_time(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "\n",
    "#### Memory\n",
    "\n",
    "En este último caso, se requiere encontrar menciones a otros usuarios dentro de cada publicación, lo cual se puede realizar buscando el caracter '@' dentro del contenido de cada una de ellas en consideración que son publicaciones de twitter. Al igual que en los problemas de memoria anteriores se resolvió utilizando contadores, lo cual permite utilizar la función sorted() para ordenarlos y obtener el top 10 requerido. Para detectar el caracter '@' se utilizó la librería re de Python que permite compilar expresiones regulares y así buscar patrones de menciones de usuario dentro del contenido de la publicación.\n",
    "\n",
    "Comenzando por inicializar el diccionario que almacenará los contadores de cada usuario mencionado, se prosigue con la compilación de la expresión regular '(@\\w+)' la cual capturará cualquier combinación alfanumérica incluyendo también el caracter underscore, los cuales justamente comprenden los caracteres permitidos en los handlers de twitter, esta expresión regular permitirá encontrar menciones dentro del contenido de la publicación luego de procesar el archivo como se ha venido haciendo en la resolución de los problemas con enfoque de memoria, es decir registro a registro con un for loop luego de abrirlos con la función open(), luego por cada linea se utiliza la función json.loads() para convertirla en un objeto json y aplicar la función findall() de la expresión regular compilada en el inicio para encontrar todas las menciones dentro del contenido del tweet, específicamente encontrado en el valor de la llave 'content' dentro del objeto json. A continuación, por cada usuario encontrado se aumenta su contador respectivo dentro del diccionario de usuarios mencionados según corresponda. Finalmente, utilizando la función sorted() sobre el diccionario de usuarios mencionados se obtiene el top 10 histórico de usuarios más influyentes, utilizando una comprensión de lista para generar el output requerido, correspondiendo a una lista de tuplas cuyos valores son cada usuario con su cantidad de menciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of mentioned users\n",
    "    mentionedUsersCount = {}\n",
    "\n",
    "    #Compile a regex to find '@' mentions\n",
    "    regex = compile(r'(@\\w+)')\n",
    "\n",
    "    #Read file, by line\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            #Find mentions on tweet\n",
    "            mentionedUsers = regex.findall(json.loads(line)['content'])\n",
    "\n",
    "            #Add users to the tracker\n",
    "            for mentionedUser in mentionedUsers:\n",
    "                mentionedUsersCount[mentionedUser] = mentionedUsersCount.get(mentionedUser, 0) + 1\n",
    "\n",
    "    #Sort mentioned users by count then return\n",
    "    sortedMentionedUsers = sorted(mentionedUsersCount, key=lambda e: mentionedUsersCount[e], reverse=True)[0:10]\n",
    "    return [(mentionedUser, mentionedUsersCount[mentionedUser]) for mentionedUser in sortedMentionedUsers]\n",
    "\n",
    "print(\n",
    "    q3_memory(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time\n",
    "\n",
    "Finalmente, se utilizó también la capacidad de las expresiones regulares para encontrar menciones dentro del contenido de cada tweet, diferenciándose del enfoque en memoria respecto a la forma en la cual se lee el archivo y las funciones utilizadas para procesar los datos, los valores obtenidos y realizar el top requerido.\n",
    "\n",
    "Se inicializa una lista de usuarios mencionados que almacenará cada una de las menciones encontradas en los pasos siguientes, luego, se compila la expresión regular '(@\\w+)' la cual cumple la función ya conocida. A continuación de almacenar en una variable str el contenido del archivo, se repite el procesamiento realizado en las respuestas con enfoque de tiempo anteriores, dividiendo la variable str en una lista con la función splitlines() cuyos elementos son procesados por la función json.loads() a través de la función map, resultando en un iterable que pueda ser procesado por un for loop el cual permitirá el uso de la función findall() de la expresión regular compilada en un inicio, con el fin de encontrar las menciones en el contenido del tweet disponible dentro del campo 'content' de cada registro, para así de esta forma agregar cada mención encontrada a la lista de usuarios mencionados. Finalmente, se transforma la lista de usuarios mencionados en una variable tipo Counter, permitiendo utilizar su función most_common(10) para así obtener el top 10 histórico de usuarios más influyentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@narendramodi', 2261), ('@Kisanektamorcha', 1836), ('@RakeshTikaitBKU', 1639), ('@PMOIndia', 1422), ('@RahulGandhi', 1125), ('@GretaThunberg', 1046), ('@RaviSinghKA', 1015), ('@rihanna', 972), ('@UNHumanRights', 962), ('@meenaharris', 925)]\n"
     ]
    }
   ],
   "source": [
    "def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
    "    #Keep track of mentioned users\n",
    "    mentionedUsers = []\n",
    "\n",
    "    #Compile a regex to find '@' mentions\n",
    "    regex = compile(r'(@\\w+)')\n",
    "\n",
    "    #Read whole file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        rawContent = f.read()\n",
    "\n",
    "    #Split the file by newlines\n",
    "    for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
    "        #Parse tweet, find mentioned users and add them to the tracker\n",
    "        mentionedUsers += regex.findall(tweet['content'])\n",
    "\n",
    "    return Counter(mentionedUsers).most_common(10)\n",
    "\n",
    "print(\n",
    "    q3_time(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "Con el código mencionado en consideración, se utilizaron las librerías memory-profiler y cProfile para realizar los respectivos benchmark de cada una de las respuestas.\n",
    "\n",
    "* IMPORTANTE: Setear el nombre del ejecutable de Python en el bloque de código siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark setup\n",
    "\n",
    "pythonExecutable = 'python3'\n",
    "\n",
    "## Imports\n",
    "\n",
    "import cProfile\n",
    "import subprocess\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "No hay mucho que indicar respecto al uso de memoria más que destacar que efectivamente se puede apreciar un menor uso de memoria en el enfoque dirigido a este cometido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     42.8 MiB     42.8 MiB           1   @profile\n",
      "     8                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "     9                                             #Keep track of tweet count by date\n",
      "    10     42.8 MiB      0.0 MiB           1       tweetCountByDate = {}\n",
      "    11                                         \n",
      "    12                                             #Keep track of user tweet count by date\n",
      "    13     42.8 MiB      0.0 MiB           1       tweetUsersByDate = {}\n",
      "    14                                         \n",
      "    15                                             #Read file, by line\n",
      "    16     42.8 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    17     49.4 MiB      0.6 MiB      117408           for line in f:\n",
      "    18                                                     #Parse line, get username and date\n",
      "    19     49.4 MiB      3.5 MiB      117407               parsedLine = json.loads(line)\n",
      "    20     49.4 MiB      0.0 MiB      117407               tweetDate = date.fromisoformat(parsedLine['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
      "    21     49.4 MiB      0.0 MiB      117407               tweetUsername = parsedLine['user']['username']\n",
      "    22                                         \n",
      "    23                                                     #Add values to trackers\n",
      "    24     49.4 MiB      0.0 MiB      117407               tweetCountByDate[tweetDate] = tweetCountByDate.get(tweetDate, 0) + 1\n",
      "    25     49.4 MiB      0.0 MiB      117407               tweetUsersByDate[tweetDate] = tweetUsersByDate.get(tweetDate, {})\n",
      "    26     49.4 MiB      2.5 MiB      117407               tweetUsersByDate[tweetDate][tweetUsername] = tweetUsersByDate[tweetDate].get(tweetUsername, 0) + 1\n",
      "    27                                         \n",
      "    28                                                 #Sort dates by tweet count then for each date sort their respective users to get the top one\n",
      "    29     49.4 MiB      0.0 MiB           1           output = []\n",
      "    30     49.4 MiB      0.0 MiB          27           sortedDates = sorted(tweetCountByDate, key=lambda d: tweetCountByDate[d], reverse=True)[0:10]\n",
      "    31     49.5 MiB      0.0 MiB          11           for dateId in sortedDates:\n",
      "    32     49.5 MiB      0.1 MiB       88328               mostTweetsUser = sorted(tweetUsersByDate[dateId], key=lambda username: tweetUsersByDate[dateId][username], reverse=True)[0]\n",
      "    33     49.5 MiB      0.0 MiB          10               output.append((dateId, mostTweetsUser))\n",
      "    34                                         \n",
      "    35     49.5 MiB      0.0 MiB           1           return output\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_memory\n",
    "\n",
    "q1MemoryResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q1_memory.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q1MemoryResult.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q1_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     42.3 MiB     42.3 MiB           1   @profile\n",
      "     9                                         def q1_time(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    10                                             #Open the whole file\n",
      "    11     42.3 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    12    431.1 MiB    388.8 MiB           1           rawContent = f.read()\n",
      "    13                                         \n",
      "    14                                             #Keep track of dates\n",
      "    15    431.1 MiB      0.0 MiB           1       dates = []\n",
      "    16                                         \n",
      "    17                                             #Keep a list for each date, holding the username for each tweet on that date\n",
      "    18    431.1 MiB      0.0 MiB           1       usersByDate = {}\n",
      "    19                                         \n",
      "    20                                             #Split the file by newlines\n",
      "    21    870.3 MiB     29.7 MiB      117408       for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
      "    22                                                 #Parse tweet, get username and date, add them to the trackers\n",
      "    23    870.3 MiB      0.0 MiB      117407           tweetDate = date.fromisoformat(tweet['date'].split('T')[0]) #Didn't use strptime due to local Python version\n",
      "    24    870.3 MiB      0.9 MiB      117407           dates.append(tweetDate)\n",
      "    25    870.3 MiB      0.0 MiB      117407           tweetUsername = tweet['user']['username']\n",
      "    26    870.3 MiB      0.0 MiB      117407           if tweetDate in usersByDate:\n",
      "    27    870.3 MiB      0.5 MiB      117394               usersByDate[tweetDate].append(tweetUsername)\n",
      "    28                                                 else:\n",
      "    29    868.9 MiB      0.0 MiB          13               usersByDate[tweetDate] = [tweetUsername]\n",
      "    30                                         \n",
      "    31                                             #Get the dates with most tweets\n",
      "    32    462.3 MiB   -408.0 MiB           1       mostTweetedDates = Counter(dates).most_common(10)\n",
      "    33                                         \n",
      "    34                                             #For the top 10 dates with most tweet counts get the most active user\n",
      "    35    462.3 MiB      0.0 MiB           1       output = []\n",
      "    36    462.8 MiB      0.0 MiB          11       for tweetDate in mostTweetedDates:\n",
      "    37    462.8 MiB      0.0 MiB          10           dateId = tweetDate[0]\n",
      "    38    462.8 MiB      0.6 MiB          10           output.append((dateId, Counter(usersByDate[dateId]).most_common(1)[0][0]))\n",
      "    39                                         \n",
      "    40    462.8 MiB      0.0 MiB           1       return output\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_time\n",
    "\n",
    "q1TimeResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q1_time.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q1TimeResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     52.9 MiB     52.9 MiB           1   @profile\n",
      "     8                                         def q2_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     9                                             #Keep track of emojis\n",
      "    10     52.9 MiB      0.0 MiB           1       emojiCount = {}\n",
      "    11                                         \n",
      "    12                                             #Read file, by line\n",
      "    13     52.9 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    14     54.3 MiB      0.9 MiB      117408           for line in f:\n",
      "    15                                                     #Parse line, find emojis and add them to the tracker\n",
      "    16     54.3 MiB      0.4 MiB    17386304               emojis = [c for c in json.loads(line)['content'] if c in EMOJI_DATA]\n",
      "    17     54.3 MiB      0.0 MiB      163043               for emoji in emojis:\n",
      "    18     54.3 MiB      0.0 MiB       45636                   emojiCount[emoji] = emojiCount.get(emoji, 0) + 1\n",
      "    19                                         \n",
      "    20                                             #Sort emojis by count\n",
      "    21     54.3 MiB      0.0 MiB        1283       sortedEmojis = sorted(emojiCount, key=lambda e: emojiCount[e], reverse=True)[0:10]\n",
      "    22     54.3 MiB      0.0 MiB          13       return [(emoji, emojiCount[emoji]) for emoji in sortedEmojis]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_memory\n",
    "\n",
    "q2MemoryResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q2_memory.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q2MemoryResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q2_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     53.2 MiB     53.2 MiB           1   @profile\n",
      "     9                                         def q2_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    10                                             #Keep track of emojis\n",
      "    11     53.2 MiB      0.0 MiB           1       tweetedEmojis = []\n",
      "    12                                         \n",
      "    13                                             #Read whole file\n",
      "    14     53.2 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    15    435.6 MiB    382.5 MiB           1           rawContent = f.read()\n",
      "    16                                         \n",
      "    17                                             #Split the file by newlines\n",
      "    18    863.5 MiB -2858109.1 MiB      117408       for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
      "    19                                                 #Parse tweet, find emojis and add them to the tracker\n",
      "    20    863.5 MiB -408950014.3 MiB    17386304           tweetedEmojis += [c for c in tweet['content'] if c in EMOJI_DATA]\n",
      "    21                                         \n",
      "    22    396.6 MiB   -467.0 MiB           1       return Counter(tweetedEmojis).most_common(10)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_time\n",
    "\n",
    "q2TimeResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q2_time.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q2TimeResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7     42.3 MiB     42.3 MiB           1   @profile\n",
      "     8                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "     9                                             #Keep track of mentioned users\n",
      "    10     42.3 MiB      0.0 MiB           1       mentionedUsersCount = {}\n",
      "    11                                         \n",
      "    12                                             #Compile a regex to find '@' mentions\n",
      "    13     42.3 MiB      0.0 MiB           1       regex = compile(r'(@\\w+)')\n",
      "    14                                         \n",
      "    15                                             #Read file, by line\n",
      "    16     42.3 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    17     45.7 MiB      1.0 MiB      117408           for line in f:\n",
      "    18                                                     #Find mentions on tweet\n",
      "    19     45.7 MiB      1.3 MiB      117407               mentionedUsers = regex.findall(json.loads(line)['content'])\n",
      "    20                                         \n",
      "    21                                                     #Add users to the tracker\n",
      "    22     45.7 MiB      0.0 MiB      221473               for mentionedUser in mentionedUsers:\n",
      "    23     45.7 MiB      1.1 MiB      104066                   mentionedUsersCount[mentionedUser] = mentionedUsersCount.get(mentionedUser, 0) + 1\n",
      "    24                                         \n",
      "    25                                             #Sort mentioned users by count then return\n",
      "    26     46.0 MiB      0.2 MiB       31351       sortedMentionedUsers = sorted(mentionedUsersCount, key=lambda e: mentionedUsersCount[e], reverse=True)[0:10]\n",
      "    27     46.0 MiB      0.0 MiB          13       return [(mentionedUser, mentionedUsersCount[mentionedUser]) for mentionedUser in sortedMentionedUsers]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_memory\n",
    "\n",
    "q3MemoryResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q3_memory.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q3MemoryResult.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: benchmark/q3_time.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     8     42.7 MiB     42.7 MiB           1   @profile\n",
      "     9                                         def q3_time(file_path: str) -> List[Tuple[str, int]]:\n",
      "    10                                             #Keep track of mentioned users\n",
      "    11     42.7 MiB      0.0 MiB           1       mentionedUsers = []\n",
      "    12                                         \n",
      "    13                                             #Compile a regex to find '@' mentions\n",
      "    14     42.7 MiB      0.0 MiB           1       regex = compile(r'(@\\w+)')\n",
      "    15                                         \n",
      "    16                                             #Read whole file\n",
      "    17     42.7 MiB      0.0 MiB           1       with open(file_path, \"r\") as f:\n",
      "    18    431.4 MiB    388.8 MiB           1           rawContent = f.read()\n",
      "    19                                         \n",
      "    20                                             #Split the file by newlines\n",
      "    21    865.4 MiB     25.0 MiB      117408       for tweet in map(json.loads, rawContent.splitlines()): #Using splitlines will allow to skip newlines which may be present on the tweet's content\n",
      "    22                                                 #Parse tweet, find mentioned users and add them to the tracker\n",
      "    23    865.4 MiB      1.0 MiB      117407           mentionedUsers += regex.findall(tweet['content'])\n",
      "    24                                         \n",
      "    25    458.6 MiB   -406.8 MiB           1       return Counter(mentionedUsers).most_common(10)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_time\n",
    "\n",
    "q3TimeResult = subprocess.run([pythonExecutable, '-m', 'memory_profiler', 'benchmark/q3_time.py', file_path], stdout=subprocess.PIPE, text=True)\n",
    "print(q3TimeResult.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time\n",
    "\n",
    "Si bien los resultados relativos al tiempo dependen del procesamiento realizado de cara a resolver cada uno de los problemas propuestos, cabe destacar que se puede verificar lo indicado en el apartado Time del Overview de este documento, donde se puede apreciar que la metodología planteada para resolver los problemas con enfoque de tiempo se demora levemente más que la metodología propuesta para resolver los problemas con enfoque en uso de memoria, lo cual podría ser un indicio que un mejor criterio para crear código de mayor calidad sería minimizar el uso de funciones de alto nivel como se realizó en este caso, específicamente map(), splitlines() y most_common(), ya que como se puede apreciar en la metodología del enfoque en memoria, se pueden tener mejores tiempos con un mucho menor uso de recursos. Por otro lado, es importante destacar la disminución de llamadas a funciones que se obtiene con la metodología con enfoque de tiempo que se puede verificar en los benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1904852 function calls in 2.284 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.509    0.509    2.284    2.284 618638651.py:1(q1_memory)\n",
      "       13    0.000    0.000    0.000    0.000 618638651.py:23(<lambda>)\n",
      "    44159    0.002    0.000    0.002    0.000 618638651.py:25(<lambda>)\n",
      "        1    0.000    0.000    2.284    2.284 <string>:1(<module>)\n",
      "   117407    0.045    0.000    1.680    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.014    0.000    0.039    0.000 codecs.py:319(decode)\n",
      "   117407    0.069    0.000    1.619    0.000 decoder.py:332(decode)\n",
      "   117407    1.497    0.000    1.497    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "    49772    0.025    0.000    0.025    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.000    0.000    2.284    2.284 {built-in method builtins.exec}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "       11    0.004    0.000    0.006    0.001 {built-in method builtins.sorted}\n",
      "   117407    0.007    0.000    0.007    0.000 {built-in method fromisoformat}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.010    0.000    0.010    0.000 {method 'end' of 're.Match' objects}\n",
      "   352221    0.030    0.000    0.030    0.000 {method 'get' of 'dict' objects}\n",
      "   234814    0.038    0.000    0.038    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'split' of 'str' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_memory\n",
    "cProfile.run('q1_memory(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1643837 function calls in 2.683 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.199    0.199    2.677    2.677 3097206275.py:1(q1_time)\n",
      "        1    0.007    0.007    2.683    2.683 <string>:1(<module>)\n",
      "   117407    0.041    0.000    1.666    0.000 __init__.py:299(loads)\n",
      "       11    0.000    0.000    0.010    0.001 __init__.py:581(__init__)\n",
      "       11    0.000    0.000    0.001    0.000 __init__.py:600(most_common)\n",
      "       11    0.000    0.000    0.010    0.001 __init__.py:649(update)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "       11    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "        1    0.000    0.000    0.068    0.068 codecs.py:319(decode)\n",
      "   117407    0.065    0.000    1.611    0.000 decoder.py:332(decode)\n",
      "   117407    1.498    0.000    1.498    0.000 decoder.py:343(raw_decode)\n",
      "       11    0.000    0.000    0.001    0.000 heapq.py:521(nlargest)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:563(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:577(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 interactiveshell.py:301(_modified_open)\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.068    0.068    0.068    0.068 {built-in method _codecs.utf_8_decode}\n",
      "       11    0.010    0.001    0.010    0.001 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _heapq.heapreplace}\n",
      "        1    0.000    0.000    2.683    2.683 {built-in method builtins.exec}\n",
      "   117418    0.005    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
      "       11    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "       10    0.001    0.000    0.001    0.000 {built-in method builtins.max}\n",
      "   117407    0.007    0.000    0.007    0.000 {built-in method fromisoformat}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "   234811    0.009    0.000    0.009    0.000 {method 'append' of 'list' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.010    0.000    0.010    0.000 {method 'end' of 're.Match' objects}\n",
      "       11    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "   234814    0.032    0.000    0.032    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        1    0.466    0.466    0.534    0.534 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'split' of 'str' objects}\n",
      "        1    0.238    0.238    0.238    0.238 {method 'splitlines' of 'str' objects}\n",
      "   117407    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q1_time\n",
    "cProfile.run('q1_time(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1437310 function calls in 2.665 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.437    0.437    2.665    2.665 2931744933.py:1(q2_memory)\n",
      "      641    0.000    0.000    0.000    0.000 2931744933.py:14(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 2931744933.py:15(<listcomp>)\n",
      "   117407    0.515    0.000    0.515    0.000 2931744933.py:9(<listcomp>)\n",
      "        1    0.000    0.000    2.665    2.665 <string>:1(<module>)\n",
      "   117407    0.048    0.000    1.670    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.014    0.000    0.039    0.000 codecs.py:319(decode)\n",
      "   117407    0.067    0.000    1.607    0.000 decoder.py:332(decode)\n",
      "   117407    1.490    0.000    1.490    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "    49772    0.025    0.000    0.025    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.000    0.000    2.665    2.665 {built-in method builtins.exec}\n",
      "   117407    0.004    0.000    0.004    0.000 {built-in method builtins.isinstance}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "    45636    0.003    0.000    0.003    0.000 {method 'get' of 'dict' objects}\n",
      "   234814    0.037    0.000    0.037    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_memory\n",
    "cProfile.run('q2_memory(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1291513 function calls in 2.971 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.156    0.156    2.966    2.966 2183131300.py:1(q2_time)\n",
      "   117407    0.499    0.000    0.499    0.000 2183131300.py:12(<listcomp>)\n",
      "        1    0.005    0.005    2.971    2.971 <string>:1(<module>)\n",
      "   117407    0.045    0.000    1.624    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:581(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:600(most_common)\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:649(update)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "        1    0.000    0.000    0.054    0.054 codecs.py:319(decode)\n",
      "   117407    0.062    0.000    1.565    0.000 decoder.py:332(decode)\n",
      "   117407    1.459    0.000    1.459    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:521(nlargest)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:563(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:577(<listcomp>)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.054    0.054    0.054    0.054 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method _heapq.heapreplace}\n",
      "        1    0.000    0.000    2.971    2.971 {built-in method builtins.exec}\n",
      "   117408    0.004    0.000    0.004    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "   234814    0.031    0.000    0.031    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        1    0.386    0.386    0.440    0.440 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "        1    0.244    0.244    0.244    0.244 {method 'splitlines' of 'str' objects}\n",
      "   117407    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q2_time\n",
    "cProfile.run('q2_time(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1510777 function calls in 2.197 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.438    0.438    2.197    2.197 717031050.py:1(q3_memory)\n",
      "    15675    0.001    0.000    0.001    0.000 717031050.py:19(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 717031050.py:20(<listcomp>)\n",
      "        1    0.000    0.000    2.197    2.197 <string>:1(<module>)\n",
      "   117407    0.044    0.000    1.663    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "    49772    0.014    0.000    0.038    0.000 codecs.py:319(decode)\n",
      "   117407    0.066    0.000    1.604    0.000 decoder.py:332(decode)\n",
      "   117407    1.487    0.000    1.487    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:250(compile)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:289(_compile)\n",
      "    49772    0.025    0.000    0.025    0.000 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.000    0.000    2.197    2.197 {built-in method builtins.exec}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.isinstance}\n",
      "   117407    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.002    0.002    0.002    0.002 {built-in method builtins.sorted}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.035    0.000    0.035    0.000 {method 'findall' of 're.Pattern' objects}\n",
      "   104066    0.020    0.000    0.020    0.000 {method 'get' of 'dict' objects}\n",
      "   234814    0.036    0.000    0.036    0.000 {method 'match' of 're.Pattern' objects}\n",
      "   117407    0.011    0.000    0.011    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_memory\n",
    "cProfile.run('q3_memory(file_path)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         1291529 function calls in 2.617 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.146    0.146    2.612    2.612 2747271002.py:1(q3_time)\n",
      "        1    0.005    0.005    2.617    2.617 <string>:1(<module>)\n",
      "   117407    0.040    0.000    1.611    0.000 __init__.py:299(loads)\n",
      "        1    0.000    0.000    0.006    0.006 __init__.py:581(__init__)\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:600(most_common)\n",
      "        1    0.000    0.000    0.006    0.006 __init__.py:649(update)\n",
      "        1    0.000    0.000    0.000    0.000 _bootlocale.py:33(getpreferredencoding)\n",
      "        1    0.000    0.000    0.000    0.000 abc.py:117(__instancecheck__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
      "        1    0.000    0.000    0.075    0.075 codecs.py:319(decode)\n",
      "   117407    0.062    0.000    1.557    0.000 decoder.py:332(decode)\n",
      "   117407    1.452    0.000    1.452    0.000 decoder.py:343(raw_decode)\n",
      "        1    0.001    0.001    0.001    0.001 heapq.py:521(nlargest)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:563(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 heapq.py:577(<listcomp>)\n",
      "        1    0.000    0.000    0.001    0.001 interactiveshell.py:301(_modified_open)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:250(compile)\n",
      "        1    0.000    0.000    0.000    0.000 re.py:289(_compile)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _abc._abc_instancecheck}\n",
      "        1    0.075    0.075    0.075    0.075 {built-in method _codecs.utf_8_decode}\n",
      "        1    0.006    0.006    0.006    0.006 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _heapq.heapify}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method _heapq.heapreplace}\n",
      "        1    0.000    0.000    2.617    2.617 {built-in method builtins.exec}\n",
      "   117409    0.004    0.000    0.004    0.000 {built-in method builtins.isinstance}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "   117408    0.005    0.000    0.005    0.000 {built-in method builtins.len}\n",
      "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
      "        1    0.001    0.001    0.001    0.001 {method '__exit__' of '_io._IOBase' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "   234814    0.009    0.000    0.009    0.000 {method 'end' of 're.Match' objects}\n",
      "   117407    0.032    0.000    0.032    0.000 {method 'findall' of 're.Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "   234814    0.030    0.000    0.030    0.000 {method 'match' of 're.Pattern' objects}\n",
      "        1    0.487    0.487    0.562    0.562 {method 'read' of '_io.TextIOWrapper' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sort' of 'list' objects}\n",
      "        1    0.252    0.252    0.252    0.252 {method 'splitlines' of 'str' objects}\n",
      "   117407    0.010    0.000    0.010    0.000 {method 'startswith' of 'str' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# q3_time\n",
    "cProfile.run('q3_time(file_path)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud\n",
    "\n",
    "La simplicidad y modularidad del código provisto permite un fácil despliegue en la nube, al ser cada función un script simple, se puede desplegar sin mucho esfuerzo en un servicio como GCP Cloud Functions, o bien, puede ser agregado a una imagen Docker con la finalidad de ejecutarlo en algún servicio de despliegue de plataformas Kubernetes, lo cual está disponible en todos los proveedores Cloud con mayor presencia en el mercado, inclusive, se puede utilizar también una imagen Docker para desplegar el código en servicios más específicos como lo es GCP Cloud Run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cierre\n",
    "\n",
    "Para finalizar, a nivel personal, me pareció una experiencia importante darme cuenta en el proceso de realizar el challenge que había obtenido resultados dispares a lo esperado, y dado este caso, preferí explayarme respecto a estos hallazgos en vez de tratar de llegar a un resultado específico mediante el código, creo que es una experiencia que ameritaba ser escrita y que me permitió volcar mis ideas de una forma que me deja conforme, de la misma forma, espero haber logrado traspasar estas ideas de manera clara tanto en sus descripciones como en el código mismo.\n",
    "\n",
    "Gracias!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
